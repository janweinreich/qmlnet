{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/qmlcode/tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jan/projects/qmlnet/tutorial\n",
      "exercise_2_1.py  exercise_2_4.py  modelnew.h5  QML_Tutorial.ipynb\n",
      "exercise_2_2.py  hof_qm7.txt\t  __pycache__  QML_Tutorial_sklearn.ipynb\n",
      "exercise_2_3.py  LICENSE\t  qm7\t       tutorial_data.py\n"
     ]
    }
   ],
   "source": [
    "%cd tutorial\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1633.28 -1212.67 -1531.73 ... -1728.7  -1506.76 -1866.88]\n",
      "[[1.         0.8152882  0.87350271 ... 0.90991951 0.82913994 0.77672171]\n",
      " [0.8152882  1.         0.81460855 ... 0.84226491 0.97323939 0.78838225]\n",
      " [0.87350271 0.81460855 1.         ... 0.8913725  0.83339596 0.78083487]\n",
      " ...\n",
      " [0.90991951 0.84226491 0.8913725  ... 1.         0.86685092 0.88326791]\n",
      " [0.82913994 0.97323939 0.83339596 ... 0.86685092 1.         0.80932061]\n",
      " [0.77672171 0.78838225 0.78083487 ... 0.88326791 0.80932061 1.        ]]\n",
      "15.00718369095469\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import qml\n",
    "from qml.kernels import gaussian_kernel\n",
    "from qml.math import cho_solve\n",
    "\n",
    "from tutorial_data import compounds\n",
    "from tutorial_data import energy_pbe0\n",
    "from tutorial_data import energy_delta\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # For every compound generate a coulomb matrix\n",
    "    for mol in compounds:\n",
    "\n",
    "        mol.generate_coulomb_matrix(size=23, sorting=\"row-norm\")\n",
    "        # mol.generate_bob(size=23, asize={\"O\":3, \"C\":7, \"N\":3, \"H\":16, \"S\":1})\n",
    "            \n",
    "\n",
    "    # Make a big 2D array with all the \n",
    "    X = np.array([mol.representation for mol in compounds], dtype=np.float32)\n",
    "    energy_pbe0 = np.array(energy_pbe0,  dtype=np.float32)\n",
    "    # X = np.array([mol.bob for mol in compounds])\n",
    "\n",
    "    print(energy_pbe0)\n",
    "\n",
    "    # Assign 1000 first molecules to the training set\n",
    "    X_training = X[:1000]\n",
    "    Y_training = energy_pbe0[:1000]\n",
    "\n",
    "    # Y_training = energy_delta[:1000]\n",
    "\n",
    "    # Assign 1000 first molecules to the training set\n",
    "    X_test = X[-1000:]\n",
    "    Y_test = energy_pbe0[-1000:]\n",
    "    # Y_test = energy_delta[-1000:]\n",
    "   \n",
    "    # Calculate the Gaussian kernel\n",
    "    sigma = 100 #700.0\n",
    "    K = gaussian_kernel(X_training, X_training, sigma)\n",
    "    print(K)\n",
    "\n",
    "    # Add a small lambda to the diagonal of the kernel matrix\n",
    "    K[np.diag_indices_from(K)] += 1e-8\n",
    "\n",
    "    # Use the built-in Cholesky-decomposition to solve\n",
    "    alpha = cho_solve(K, Y_training) \n",
    "\n",
    "    #print(alpha)\n",
    "\n",
    "    # Assign 1000 last molecules to the test set\n",
    "    X_test = X[-1000:]\n",
    "    Y_test = energy_pbe0[-1000:]\n",
    "\n",
    "    # calculate a kernel matrix between test and training data, using the same sigma\n",
    "    Ks = gaussian_kernel(X_test, X_training, sigma)\n",
    "\n",
    "    # Make the predictions\n",
    "    Y_predicted = np.dot(Ks, alpha)\n",
    "\n",
    "    # Calculate mean-absolute-error (MAE):\n",
    "    print(np.mean(np.abs(Y_predicted - Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "[CV 1/3; 1/288] START LAMBDA=1e-07, decay=1e-06, dense_nparams=300, filters=10, lr=1e-06, optimizer=Adam\n",
      "[CV 1/3; 1/288] END LAMBDA=1e-07, decay=1e-06, dense_nparams=300, filters=10, lr=1e-06, optimizer=Adam; total time=  49.1s\n",
      "[CV 2/3; 1/288] START LAMBDA=1e-07, decay=1e-06, dense_nparams=300, filters=10, lr=1e-06, optimizer=Adam\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import (Input, Convolution1D, Dense, MaxPooling1D,MaxPooling2D,\n",
    "                                    Flatten, Dropout, Activation, average,\n",
    "                                    BatchNormalization, Reshape, Conv2D)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "X_training  = tf.convert_to_tensor(X_training)\n",
    "X_test      = tf.convert_to_tensor(X_test)\n",
    "Y_training  = tf.convert_to_tensor(Y_training)\n",
    "Y_test      = tf.convert_to_tensor(Y_test)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(optimizer, dense_nparams, lr, decay, LAMBDA, filters): #, ):\n",
    "    #filters = 50\n",
    "    \n",
    "\n",
    "    init = 'he_normal'\n",
    "    \n",
    "    \n",
    "    regu = tf.keras.regularizers.l2(LAMBDA)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(276, input_dim=276, kernel_initializer=init,kernel_regularizer=regu ,activation='linear'))\n",
    "    #276 = 12 x 23 \n",
    "    #BatchNormalization()\n",
    "    \n",
    "    Reshape((12, 23, 1), input_shape=(276,))\n",
    "    Conv2D(filters, 5, activation = 'selu', padding = 'same', input_shape= [12, 23, 1] ),\n",
    "    MaxPooling2D(2)\n",
    "    Conv2D(filters*2, 5, activation = 'selu', padding = 'same' ),\n",
    "    Conv2D(filters*2, 5, activation = 'selu', padding = 'same' ),\n",
    "    MaxPooling2D(2)\n",
    "    Conv2D(filters*3, 5, activation = 'selu', padding = 'same' ),\n",
    "    Conv2D(filters*3, 5, activation = 'selu', padding = 'same' ),\n",
    "    MaxPooling2D(2)\n",
    "\n",
    "    Flatten()\n",
    "    #BatchNormalization()\n",
    "    #model.add(exponential_layer)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.8)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.5)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.4)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.1)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.1)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='linear'))\n",
    "    model.add(Dense(1, activation='linear', name='Output_Energy'))\n",
    "\n",
    "    opt = Adam(lr=lr ,decay=decay)\n",
    "    model.compile(loss='mae', optimizer=opt)\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#epochs = 300 \n",
    "kears_estimator = KerasRegressor(build_fn=create_model , epochs=400, batch_size=25, verbose=0)\n",
    "\n",
    "\n",
    "'''\n",
    "GridSearchCV(estimator=SVC(),\n",
    "             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf'\n",
    "'''\n",
    "\n",
    "param_grid = {\n",
    "    'dense_nparams': [300, 400],\n",
    "    'optimizer':['Adam'], 'lr': [1e-6, 1e5, 1e-4, 1e-3], 'decay': [1e-6, 1e-5,1e-4, 1e-3], 'LAMBDA':  [1e-7, 1e-6, 1e-5], 'filters':  [10, 50, 100] }\n",
    "\n",
    "#65.587196 using {'decay': 0.0001, 'dense_nparams': 500, 'lr': 0.0001, 'optimizer': 'Adam'}\n",
    "\n",
    "kfold_splits = 3\n",
    "\n",
    "#RandomizedSearchCV\n",
    "\n",
    "search = GridSearchCV(estimator=kears_estimator,  param_grid=param_grid,cv=kfold_splits, return_train_score=True, verbose=10)\n",
    "#RandomizedSearchCV(model, space, n_iter=500, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1)\n",
    "\n",
    "\n",
    "my_callbacks =  [keras.callbacks.EarlyStopping(patience=30, monitor=\"val_loss\"), keras.callbacks.TerminateOnNaN() ]\n",
    "#https://medium.com/@am.benatmane/keras-hyperparameter-tuning-using-sklearn-pipelines-grid-search-with-cross-validation-ccfc74b0ce9f\n",
    "\n",
    "grid_result = search.fit(X_training, Y_training, callbacks= my_callbacks,validation_data=(X_test, Y_test) ) \n",
    "\n",
    "#,n_jobs=-1\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print (\"rest\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV(estimator=kears_estimator,  param_grid=param_grid,cv=kfold_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(50, 1000)\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = model #rnd_search_cv.best_estimator_\n",
    "pred = best.predict(X_test)\n",
    "pred = pred.flatten()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(pred, Y_test, \"+\")\n",
    "plt.hist(np.abs(pred - np.array(Y_test)))\n",
    "plt.show()\n",
    "np.mean(np.abs(pred - Y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "mc_predictions = []\n",
    "for i in tqdm.tqdm(range(1000)):\n",
    "    y_p = best(X_test, training=True)\n",
    "    mc_predictions.append(y_p)\n",
    "    \n",
    "mc_predictions = np.array(mc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_ensemble_pred = np.array(mc_predictions).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_ensemble_pred\n",
    "\n",
    "\n",
    "plt.hist(np.abs(mc_ensemble_pred.flatten() - np.array(Y_test)))\n",
    "plt.show()\n",
    "np.mean(np.abs(mc_ensemble_pred.flatten() - Y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
