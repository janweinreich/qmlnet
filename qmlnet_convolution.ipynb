{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/qmlcode/tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd tutorial\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import qml\n",
    "from qml.kernels import gaussian_kernel\n",
    "from qml.math import cho_solve\n",
    "\n",
    "from tutorial_data import compounds\n",
    "from tutorial_data import energy_pbe0\n",
    "from tutorial_data import energy_delta\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # For every compound generate a coulomb matrix\n",
    "    for mol in compounds:\n",
    "\n",
    "        mol.generate_coulomb_matrix(size=23, sorting=\"row-norm\")\n",
    "        # mol.generate_bob(size=23, asize={\"O\":3, \"C\":7, \"N\":3, \"H\":16, \"S\":1})\n",
    "            \n",
    "\n",
    "    # Make a big 2D array with all the \n",
    "    X = np.array([mol.representation for mol in compounds], dtype=np.float32)\n",
    "    energy_pbe0 = np.array(energy_pbe0,  dtype=np.float32)\n",
    "    # X = np.array([mol.bob for mol in compounds])\n",
    "\n",
    "    print(energy_pbe0)\n",
    "\n",
    "    # Assign 1000 first molecules to the training set\n",
    "    X_training = X[:1000]\n",
    "    Y_training = energy_pbe0[:1000]\n",
    "\n",
    "    # Y_training = energy_delta[:1000]\n",
    "\n",
    "    # Assign 1000 first molecules to the training set\n",
    "    X_test = X[-1000:]\n",
    "    Y_test = energy_pbe0[-1000:]\n",
    "    # Y_test = energy_delta[-1000:]\n",
    "   \n",
    "    # Calculate the Gaussian kernel\n",
    "    sigma = 100 #700.0\n",
    "    K = gaussian_kernel(X_training, X_training, sigma)\n",
    "    print(K)\n",
    "\n",
    "    # Add a small lambda to the diagonal of the kernel matrix\n",
    "    K[np.diag_indices_from(K)] += 1e-8\n",
    "\n",
    "    # Use the built-in Cholesky-decomposition to solve\n",
    "    alpha = cho_solve(K, Y_training) \n",
    "\n",
    "    #print(alpha)\n",
    "\n",
    "    # Assign 1000 last molecules to the test set\n",
    "    X_test = X[-1000:]\n",
    "    Y_test = energy_pbe0[-1000:]\n",
    "\n",
    "    # calculate a kernel matrix between test and training data, using the same sigma\n",
    "    Ks = gaussian_kernel(X_test, X_training, sigma)\n",
    "\n",
    "    # Make the predictions\n",
    "    Y_predicted = np.dot(Ks, alpha)\n",
    "\n",
    "    # Calculate mean-absolute-error (MAE):\n",
    "    print(np.mean(np.abs(Y_predicted - Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now make build a neural network with the coulomb matrix as input which is getting featurized by a number of\n",
    "convolutional layers and subsequently we apply a number of fully connected layers to tackle the regression problem**\n",
    "\n",
    "\n",
    "Note that for application runs you should change \n",
    "\n",
    "kears_estimator = KerasRegressor(build_fn=create_model , epochs=2,  batch_size=25, verbose=2)\n",
    "\n",
    "to \n",
    "kears_estimator = KerasRegressor(build_fn=create_model , epochs=2500,  batch_size=25, verbose=2)\n",
    "\n",
    "to get a reasonable accucacy\n",
    "\n",
    "\n",
    "\n",
    "After the cross validation (dont be suprised sklearn gives the negative MAE because they use accuracy not loss) you can get the best model\n",
    "using search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "**you should be able to get down to 15 kcal/mol just like the kernel above or maybe a little lower because the kernel version was not CV optimized on the trainingset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "from scipy.stats import reciprocal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import (Input, Convolution1D, Dense, MaxPooling1D,MaxPooling2D,AveragePooling2D,\n",
    "                                    Flatten, Dropout, Activation, average,\n",
    "                                    BatchNormalization, Reshape, Conv2D)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "#X           = np.load(\"rep.npy\", allow_pickle=True)\n",
    "#energy      = np.load(\"energy.npy\", allow_pickle=True)\n",
    "\"\"\"\n",
    "X_training  = X[:1000]\n",
    "Y_training  = energy[:1000]\n",
    "X_test      = X[-1000:]\n",
    "Y_test      = energy[-1000:]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(optimizer, dense_nparams, lr, decay, LAMBDA, filters, kernel_size): #, ):\n",
    "    #filters = 50\n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()\n",
    "\n",
    "    init = 'he_normal'\n",
    "    \n",
    "    \n",
    "    regu = tf.keras.regularizers.l2(LAMBDA)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(276, input_dim=276, kernel_initializer=init,kernel_regularizer=regu ,activation='linear'))\n",
    "    #276 = 12 x 23 \n",
    "    #BatchNormalization()\n",
    "    \n",
    "    Reshape((12, 23, 1), input_shape=(276,))\n",
    "    Conv2D(filters, kernel_size, activation = 'selu', padding = 'same', input_shape= [12, 23, 1] ),\n",
    "    MaxPooling2D(2)\n",
    "    Conv2D(filters*2, kernel_size, activation = 'selu', padding = 'same' ),\n",
    "    MaxPooling2D(2)\n",
    "\n",
    "    Flatten()\n",
    "    #BatchNormalization()\n",
    "    #model.add(exponential_layer)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "\n",
    "\n",
    "    #Dropout(rate=0.8)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.5)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.4)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.1)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.1)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='linear'))\n",
    "    model.add(Dense(1, activation='linear', name='Output_Energy'))\n",
    "\n",
    "    opt = Adam(lr=lr ,decay=decay)\n",
    "    model.compile(loss='mae', optimizer=opt)\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "# for production runs\n",
    "#kears_estimator = KerasRegressor(build_fn=create_model , epochs=2500, batch_size=25, verbose=2)\n",
    "\n",
    "# for test runs\n",
    "kears_estimator = KerasRegressor(build_fn=create_model , epochs=2,  batch_size=25, verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'dense_nparams': [1500, 2000,  2500],\n",
    "    'optimizer':['Adam', 'RMSprop'], 'lr': [0.00075,0.001, 0.0015, 0.002 ], 'decay': [0.002,0.001, 0.0007], 'LAMBDA':  [1e-8, 1e-7, 1e-6], 'filters':  [200, 250, 300, 400], 'kernel_size':[3, 5, 10, 20, 50, 60, 100] }\n",
    "\n",
    "\n",
    "kfold_splits = 5\n",
    "\n",
    "#RandomizedSearchCV\n",
    "search  = RandomizedSearchCV(estimator=kears_estimator, param_distributions=param_grid, n_iter=10, cv=kfold_splits, verbose=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "my_callbacks =  [keras.callbacks.EarlyStopping(patience=20), keras.callbacks.TerminateOnNaN() ]\n",
    "#https://medium.com/@am.benatmane/keras-hyperparameter-tuning-using-sklearn-pipelines-grid-search-with-cross-validation-ccfc74b0ce9f\n",
    "\n",
    "grid_result = search.fit(X_training, Y_training, callbacks= my_callbacks,validation_data=(X_test, Y_test) ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print (\"rest\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(50, 1000)\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = model #rnd_search_cv.best_estimator_\n",
    "pred = best.predict(X_test)\n",
    "pred = pred.flatten()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(pred, Y_test, \"+\")\n",
    "plt.hist(np.abs(pred - np.array(Y_test)))\n",
    "plt.show()\n",
    "np.mean(np.abs(pred - Y_test))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
