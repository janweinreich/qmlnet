{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "from scipy.stats import reciprocal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import (Input, Convolution1D, Dense, MaxPooling1D,MaxPooling2D,AveragePooling2D,\n",
    "                                    Flatten, Dropout, Activation, average,\n",
    "                                    BatchNormalization, Reshape, Conv2D)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from keras import backend as K\n",
    "#scoring=make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "X           = np.load(\"rep.npy\", allow_pickle=True)\n",
    "energy      = np.load(\"energy.npy\", allow_pickle=True)\n",
    "X_training  = X[:1000]\n",
    "Y_training  = energy[:1000]\n",
    "X_test      = X[-1000:]\n",
    "Y_test      = energy[-1000:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(optimizer, dense_nparams, lr, decay, LAMBDA): #, ):\n",
    "    #filters = 50\n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    init = 'he_normal'\n",
    "    \n",
    "    \n",
    "    regu = tf.keras.regularizers.l2(LAMBDA)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(276, input_dim=276, kernel_initializer=init,kernel_regularizer=regu ,activation='linear'))\n",
    "    #276 = 12 x 23 \n",
    "    BatchNormalization()\n",
    "    \n",
    "    #BatchNormalization()\n",
    "    #model.add(exponential_layer)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.8)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.5)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.4)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.1)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='selu'))\n",
    "    BatchNormalization()\n",
    "    #Dropout(rate=0.1)\n",
    "    model.add(Dense(dense_nparams, kernel_initializer=init, activation='linear'))\n",
    "    model.add(Dense(1, activation='linear', name='Output_Energy'))\n",
    "\n",
    "    opt = Adam(lr=lr ,decay=decay)\n",
    "    model.compile(loss='mae', optimizer=opt)\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#kears_estimator = KerasRegressor(build_fn=create_model , epochs=1500, batch_size=25, verbose=2)\n",
    "kears_estimator = KerasRegressor(build_fn=create_model , epochs=2500,  batch_size=25, verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "#param_grid = {\n",
    "#    'dense_nparams': [400, 500, 600, 700],\n",
    "#    'optimizer':['Adam'], 'lr': reciprocal(1e-6, 1e-2), 'decay': reciprocal(1e-5,1e-2), 'LAMBDA':  [1e-8, 1e-7], 'filters':  [2, 5, 10, 50 ], 'kernel_size':[3, 5, 10, 20, 50] }\n",
    "\n",
    "\n",
    "#param_grid = {\n",
    "#    'dense_nparams': [400, 500, 600, 700],\n",
    "#    'optimizer':['Adam'], 'lr': [1e-6, 1e-2], 'decay': [1e-5,1e-2], 'LAMBDA':  [1e-8, 1e-7], 'filters':  [2, 5, 10, 50 ], 'kernel_size':[3, 5, 10, 20, 50] }\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'dense_nparams': [3000, 3250],\n",
    "    'optimizer':['Adam'], 'lr': [0.00055, 0.0006,0.00065,0.00075 ], 'decay': [0.008, 0.005, 0.004, 0.003, 0.0025,0.002,0.001, 0.0007, 0.0005, 0.00025], 'LAMBDA':  [1e-9,1e-8, 1e-7, 1e-6] }\n",
    "#out_57:Best: -14.518220 using {'optimizer': 'Adam', 'lr': 0.0006, 'dense_nparams': 3000, 'decay': 0.0025, 'LAMBDA': 1e-0\n",
    "\n",
    "kfold_splits = 5\n",
    "\n",
    "#RandomizedSearchCV\n",
    "\n",
    "#search = GridSearchCV(estimator=kears_estimator,  param_grid=param_grid,cv=kfold_splits, verbose=1)\n",
    "#search  = RandomizedSearchCV(estimator=kears_estimator, param_distributions=param_grid, n_iter=10, cv=kfold_splits,scorer=scoring,  scoring='neg_mean_absolute_error', verbose=10)\n",
    "#search  = RandomizedSearchCV(estimator=kears_estimator, param_distributions=param_grid, n_iter=10, cv=kfold_splits,scoring=scoring, verbose=10)\n",
    "search  = RandomizedSearchCV(estimator=kears_estimator, param_distributions=param_grid, n_iter=10, cv=kfold_splits, verbose=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "my_callbacks =  [keras.callbacks.EarlyStopping(patience=100), keras.callbacks.TerminateOnNaN() ]\n",
    "#https://medium.com/@am.benatmane/keras-hyperparameter-tuning-using-sklearn-pipelines-grid-search-with-cross-validation-ccfc74b0ce9f\n",
    "\n",
    "grid_result = search.fit(X_training, Y_training, callbacks= my_callbacks,validation_data=(X_test, Y_test) ) \n",
    "\n",
    "#,n_jobs=-1\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print (\"rest\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
